gcloud beta dataproc clusters create cluster-spark --region europe-west4 --zone europe-west4-a --master-machine-type n1-standard-4 --master-boot-disk-size 100 --num-workers 2 --worker-machine-type n1-standard-2 --worker-boot-disk-size 50 --image-version 2.0-debian10 --project cn-g14-projecto



gcloud dataproc jobs submit pyspark --cluster=cluster-spark  gs://cn-spark-bucket/job.py --packages graphframes:graphframes:0.6.0-spark2.3-s_2.11

--metadata='PIP_PACKAGES=pandas==0.23.0 scipy==1.1.0' \
    --initialization-actions=gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh
POST /v1/projects/cn-g14-projecto/regions/europe-west4/jobs:submit/
{
  "projectId": "cn-g14-projecto",
  "job": {
    "placement": {
      "clusterName": "cluster-bigquery"
    },
    "statusHistory": [],
    "reference": {
      "jobId": "job-34a0db8e",
      "projectId": "cn-g14-projecto"
    },
    "pysparkJob": {
      "mainPythonFileUri": "gs://cn-spark-bucket/pyspark.py",
      "properties": {}
    }
  }
}
